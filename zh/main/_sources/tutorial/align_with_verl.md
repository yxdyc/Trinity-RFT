# ä¸ veRL å¯¹é½è®­ç»ƒé…ç½®

æœ¬æŒ‡å—ä¸ºç†Ÿæ‚‰ [veRL](https://github.com/volcengine/verl) çš„ç”¨æˆ·æä¾›äº†å°† Trinity-RFT ä¸ veRL çš„å‚æ•°å’ŒæŒ‡æ ‡å¯¹é½çš„æ–¹æ³•ã€‚

Trinity-RFT ä½¿ç”¨ [veRL](https://github.com/volcengine/verl) ä½œä¸ºè®­ç»ƒåç«¯ï¼ˆ`trainer`ï¼‰ï¼ŒåŒ…æ‹¬ actorã€reference å’Œ critic æ¨¡å‹ã€‚Trinity-RFT ä¸­çš„ `explorer` æ¨¡å—åŸºäº [vllm](https://github.com/vllm-project/vllm) å®ç°ï¼Œå–ä»£äº† veRL åŸç”Ÿçš„ rollout å¼•æ“ã€‚æ­¤å¤–ï¼ŒTrinity-RFT å¼•å…¥äº†æ–°æ¨¡å— `buffer` æ¥å¢å¼º RFT çš„å…¨ç”Ÿå‘½å‘¨æœŸæ•°æ®ç®¡ç†ï¼Œå¯ä»¥ç†è§£ä¸ºå¯¹ veRL çš„ RL dataset å’Œ DataProto çš„è¿›ä¸€æ­¥å¼ºåŒ–ã€‚

## å‚æ•°æ˜ å°„

veRL ä¸­çš„æ ¸å¿ƒå‚æ•°åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š`algorithm`ã€`data`ã€`actor_rollout_ref`ã€`critic`ã€`reward_model` å’Œ `trainer`ã€‚
Trinity-RFT æ ¹æ®åŠŸèƒ½å°†å¼ºåŒ–å¾®è°ƒçš„å¤§é‡å‚æ•°åˆ†ä¸ºå‡ ä¸ªéƒ¨åˆ†ï¼Œä¾‹å¦‚ `algorithm`ã€`model`ã€`buffer`ã€`explorer`ã€`trainer`ã€`monitor`ã€`synchronizer` å’Œ `cluster`ã€‚

å¤§è‡´æ¥è¯´ï¼ŒveRL ä¸­çš„å‚æ•°å¯ä»¥æŒ‰ç…§ä¸‹é¢çš„æ–¹å¼æ˜ å°„åˆ° Trinity-RFT ä¸­ï¼š

| é…ç½® | veRL | Trinity-RFT |
|:----------|:-----|:-----|
| ç®—æ³•ï¼Œä¾‹å¦‚ Advantage å‡½æ•° | `algorithm` | `algorithm` |
| è®­ç»ƒå’Œè¯„ä¼°ä»»åŠ¡é›† | `data` | `buffer.explorer_input` |
| æ‰¹æ¬¡å¤§å°ï¼ˆğŸ’¡ ç¨åè¯´æ˜ï¼‰ | `data.train_batch_size` å’Œ `actor_rollout_ref.actor.ppo_mini_batch_size` | `buffer.batch_size` å’Œ `buffer.train_batch_size` |
| Actor | `actor_rollout_ref.actor` | `model` å’Œ `trainer` |
| Rollout | `actor_rollout_ref.rollout` | `explorer.rollout_model` |
| Critic | `critic` | `trainer.trainer_config.critic` |
| å¥–åŠ±æ¨¡å‹ | `reward_model` | `explorer.auxiliary_models` |
| ä¸€äº›å…¨å±€é…ç½® | `trainer` | `monitor`ã€`synchronizer`ã€`cluster` ç­‰ |


åœ¨ä»¥ä¸‹å†…å®¹ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•å°† veRL ä¸­çš„å‚æ•°æ˜ å°„åˆ° Trinity-RFT ä¸­çš„å‚æ•°ã€‚æœ‰å…³ Trinity-RFT çš„è¯¦ç»†å‚æ•°é…ç½®ï¼Œè¯·å‚è€ƒ[æ–‡æ¡£](https://modelscope.github.io/Trinity-RFT/zh/main/tutorial/trinity_configs.html)ã€‚


```{note}
ä¸ºäº†åŒ¹é… veRL çš„é»˜è®¤è®­ç»ƒè®¾ç½®ï¼Œæˆ‘ä»¬åœ¨ Trinity-RFT ä¸­è®¾ç½® `synchronizer.sync_style=fixed` å’Œ `synchronizer.sync_offset=0`ã€‚
```

### Algorithm

| veRL | Trinity-RFT | è¯´æ˜ |
|:-----|:-----|:-----|
| `algorithm.adv_estimator` | `algorithm.advantage_fn` | é€šè¿‡ `algorithm.advantage_fn_args` ä¼ é€’å‚æ•° |
| `algorithm.gamma` | `algorithm.advantage_fn_args.gamma` | ä¸ `algorithm.advantage_fn: ppo/reinforceplusplus` ä¸€èµ·ä½¿ç”¨ |
| `algorithm.lam` | `algorithm.advantage_fn_args.lam` | ä¸ `algorithm.advantage_fn: ppo` ä¸€èµ·ä½¿ç”¨ |
| `algorithm.use_kl_in_reward` | `algorithm.kl_penalty_fn` | é€šè¿‡è®¾ç½® `algorithm.kl_penalty_fn=none` ç¦ç”¨å¥–åŠ±ä¸­çš„ KL |
| `algorithm.kl_penalty` | `algorithm.kl_penalty_fn` | ä» `k2`ã€`low_var_kl` ç­‰ä¸­é€‰æ‹© |
| `algorithm.kl_ctrl.kl_coef` | `algorithm.kl_penalty_fn_args.kl_coef` | - |

ğŸ’¡ è¯¦ç»†è¯´æ˜ï¼š

* åœ¨ä½¿ç”¨ä¼˜åŠ¿å‡½æ•°æˆ–ç­–ç•¥æŸå¤±å‡½æ•°çš„å‚æ•°ï¼ˆä¾‹å¦‚ `algorithm.advantage_fn_args`ï¼‰ä¹‹å‰ï¼Œå»ºè®®æ£€æŸ¥æºä»£ç ä»¥ç¡®ä¿è¿™äº›å‚æ•°èƒ½å¤Ÿè¢«ç›¸åº”å‡½æ•°æ­£ç¡®å¤„ç†ã€‚


### Data

| veRL | Trinity-RFT | è¯´æ˜ |
|:-----|:-----|:-----|
| `data.train_files` | `buffer.explorer_input.taskset.path` æˆ– `buffer.explorer_input.tasksets[i].path` | - |
| `data.val_files` | `buffer.explorer_input.eval_tasksets[i].path` | - |
| `data.prompt_key` | `buffer.explorer_input.taskset.format.prompt_key`| Taskset-specific |
| `data.response_key` | `buffer.explorer_input.taskset.format.response_key`| Taskset-specific |
| `data.train_batch_size` | `buffer.batch_size` * `synchronizer.sync_interval` | è¦æ¢ç´¢çš„ä»»åŠ¡æ•°é‡ |
| `data.val_batch_size` | `buffer.batch_size` | åœ¨ veRL ä¸­å·²å¼ƒç”¨ |
| `data.max_prompt_length` | `model.max_prompt_tokens` | - |
| `data.max_response_length` | `model.max_response_tokens` | - |
| `data.filter_overlong_prompts` | `model.enable_prompt_truncation` | ç¨åè¯´æ˜ |
| `data.truncation` | - | ç­‰åŒäº `right` |
| `data.shuffle` | `buffer.explorer_input.taskset.task_selector.selector_type:shuffle` | Taskset-specific |

ğŸ’¡ è¯¦ç»†è¯´æ˜ï¼š

* æ³¨é‡Š `taskset-specific` æ„å‘³ç€æ‚¨å¯ä»¥åœ¨ `buffer.explorer_input.tasksets[i]` æˆ– `buffer.explorer_input.eval_tasksets[i]` ä¸­ä¸ºæ¯ä¸ªè®­ç»ƒæˆ–è¯„ä¼°ä»»åŠ¡è®¾ç½®ä¸åŒçš„å‚æ•°ã€‚

* å¯¹äºä¸ `batch size` ç›¸å…³çš„å‚æ•°ï¼ŒTrinity-RFT ä½¿ç”¨ `buffer.batch_size` æ¥æ§åˆ¶æ¯ä¸ªæ¢ç´¢æ­¥éª¤ä¸­è¦æ¢ç´¢çš„ä»»åŠ¡æ•°é‡ï¼Œä½¿ç”¨ `buffer.train_batch_size` æ¥æ§åˆ¶æ¯ä¸ªæ¢¯åº¦ä¸‹é™æ­¥éª¤ä¸­ä½¿ç”¨çš„ä»»åŠ¡æ•°é‡ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ§åˆ¶ä»¥ä¸‹å‚æ•°å¯ä»¥ç¡®ä¿ä¸ veRL ç›¸åŒçš„æ•ˆæœï¼š
    - Trinity-RFT ä¸­çš„ `buffer.batch_size` = veRL ä¸­çš„ `actor_rollout_ref.actor.ppo_mini_batch_size`
    - Trinity-RFT ä¸­çš„ `buffer.train_batch_size`ï¼ˆè‡ªåŠ¨ï¼‰= veRL ä¸­çš„ `actor_rollout_ref.rollout.n` * `actor_rollout_ref.actor.ppo_mini_batch_size`
    - Trinity-RFT ä¸­çš„ `synchronizer.sync_interval` = veRL ä¸­çš„ `data.train_batch_size` / `actor_rollout_ref.actor.ppo_mini_batch_size`
    - ä¸è¦è®¾ç½® `ppo_mini_batch_size`ï¼Œå®ƒä¼šè‡ªåŠ¨è®¾ç½®ä»¥åŒ¹é… veRL çš„æ•ˆæœï¼Œå°½ç®¡å€¼å¯èƒ½ä¸åŒã€‚

* å¦‚æœæ‚¨æƒ³è¿‡æ»¤è¿‡é•¿çš„æç¤ºï¼Œå¯ä»¥åœ¨ Trinity-RFT ä¸­è®¾ç½® `model.enable_prompt_truncation=True`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›¸åº”çš„ç»éªŒå°†ä¸è®¡å…¥æŸå¤±è®¡ç®—ï¼Œå› æ­¤ `truncation` çš„æ–¹å‘ä¸å†é‡è¦ã€‚


### Actorã€Rollout å’Œ Critic

æœ¬èŠ‚åŒ…æ‹¬ actor å’Œ rollout çš„å‚æ•°ã€‚ä¸ºäº†ä¾¿äºç†è§£ï¼Œæ‚¨å¯ä»¥å°† veRL ä¸­çš„ actorï¼ˆ`actor_rollout_ref.actor`ï¼‰è§†ä¸º Trinity-RFT ä¸­çš„ trainerï¼ˆ`trainer`ï¼‰ï¼Œå°† rolloutï¼ˆ`actor_rollout_ref.rollout`ï¼‰è§†ä¸º explorerï¼ˆ`explorer.rollout_model`ï¼‰ã€‚

```{note}
Trinity-RFT ä¸­ `actor_rollout_ref.rollout` çš„ä»»ä½•å‚æ•°éƒ½æ— æ•ˆï¼›è¯·åœ¨å…¶ä»–å­—æ®µä¸­æ­£ç¡®è®¾ç½®å®ƒä»¬ã€‚
```

å¯¹äº veRL çš„é«˜çº§è®­ç»ƒé…ç½®ï¼Œæ‚¨å¯ä»¥åœ¨ `trainer.trainer_config` å­—æ®µä¸­è®¾ç½®è¿™äº›å‚æ•°ã€‚ä¾‹å¦‚ï¼ŒveRL ä¸­çš„ `actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu` ç­‰åŒäº Trinity-RFT ä¸­çš„ `trainer.trainer_config.actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu`ã€‚å¦‚æœæ‚¨æƒ³åœ¨ `trainer.trainer_config` å­—å…¸ä¸­è®¾ç½®å‚æ•°ï¼Œè¯·ä»”ç»†é˜…è¯» `trinity/common/verl_config.py` ä¸­çš„æºä»£ç ï¼


| veRL | Trinity-RFT | è¯´æ˜ |
|:-----|:-----|:-----|
| `actor_rollout_ref.model.path` | `model.model_path` | - |
| `actor_rollout_ref.actor.optim` | `algorithm.optimizer` | ä¾‹å¦‚ `lr` å’Œ `weight_decay` |
| `actor_rollout_ref.rollout.n` | `algorithm.repeat_times` | Eval taskset-specificï¼š`eval_tasksets[i].repeat_times` |
| `actor_rollout_ref.actor.ppo_mini_batch_size` | `buffer.batch_size` | æ¯ä¸ªæ¢ç´¢æ­¥éª¤ä¸­è¦æ¢ç´¢çš„ä»»åŠ¡æ•°é‡ |
| `actor_rollout_ref.actor.use_dynamic_bsz` | `trainer.use_dynamic_bsz` | - |
| `actor_rollout_ref.actor.ppo_max_token_len_per_gpu` | `trainer.max_token_len_per_gpu` | - |
| `actor_rollout_ref.actor.ulysses_sequence_parallel_size` | `trainer.ulysses_sequence_parallel_size` | actor çš„åºåˆ—å¹¶è¡Œå¤§å° |
| `actor_rollout_ref.actor.grad_clip` | `trainer.grad_clip` | actor çš„æ¢¯åº¦è£å‰ªå€¼ |
| `actor_rollout_ref.actor.use_kl_loss` | `algorithm.kl_loss_fn` | å¦‚æœè®¾ç½®ä¸º `none`ï¼Œå°†ä¸è®¡ç®— KL æ•£åº¦æŸå¤± |
| `actor_rollout_ref.rollout.gpu_memory_utilization` | `explorer.rollout_model.gpu_memory_utilization` | - |
| `actor_rollout_ref.rollout.temperature` | `model.temperature` | å¯ä»¥æ˜¯taskset-specificï¼Œä¾‹å¦‚ `buffer.explorer_input.taskset.rollout_args.temperature` |
| `actor_rollout_ref.rollout.top_p` | `model.top_p` | å¯ä»¥æ˜¯taskset-specific |
| `actor_rollout_ref.rollout.top_k` | `model.top_k` | å¯ä»¥æ˜¯taskset-specific |
| `actor_rollout_ref.rollout.tensor_model_parallel_size` | `explorer.rollout_model.tensor_parallel_size` | - |
| `actor_rollout_ref.rollout.val_kwargs` | `buffer.explorer_input.eval_tasksets[i]` | Taskset-specific |
| `critic.model.path` | `model.critic_model_path` | é»˜è®¤ä¸º `model.model_path` |

ğŸ’¡ è¯¦ç»†è¯´æ˜ï¼š

* æ³¨é‡Š `å¯ä»¥æ˜¯taskset-specific`ï¼ˆä»¥ `temperature` ä¸ºä¾‹ï¼‰æ„å‘³ç€æ‚¨å¯ä»¥ä¸ºæ‰€æœ‰ä»»åŠ¡é›†è®¾ç½® `model.temperature`ï¼Œæˆ–è€…åœ¨ `buffer.explorer_input.taskset.rollout_args.temperature` æˆ– `buffer.explorer_input.eval_tasksets[i].rollout_args.temperature` ä¸­ä¸ºæ¯ä¸ªä»»åŠ¡è®¾ç½®ä¸åŒçš„å€¼ã€‚å…·ä½“ç¤ºä¾‹å¦‚ä¸‹ï¼š
```yaml
buffer:
  explorer_input:
    eval_tasksets:
      - name: AIME2024
        storage_type: file
        path: HuggingFaceH4/aime_2024
        split: 'train'
        repeat_times: 32
        format:
          prompt_key: 'question'
          response_key: 'answer'
        rollout_args:
          temperature: 1.0
          top_p: 0.7
```

### Reward Model

Trinity-RFT æ”¯æŒé’ˆå¯¹ä»»åŠ¡é›†å®šåˆ¶çš„å¥–åŠ±å‡½æ•°ä»¥åŠå¥–åŠ±æ¨¡å‹ã€‚å¯¹äºè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œä½ å¯ä»¥é€šè¿‡è®¾ç½® `buffer.explorer_input.default_reward_fn_type` æ¥é€‰æ‹©å¯¹åº”çš„å¥–åŠ±å‡½æ•°ï¼›å¦å¤–æ‚¨å¯ä»¥è®¾ç½® `explorer.auxiliary_models` ä½œä¸º reward model å¹¶åœ¨å·¥ä½œæµä¸­ä½¿ç”¨å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œ
```yaml
buffer:
  explorer_input:
    default_reward_fn_type: 'custom_reward'
explorer:
  auxiliary_models:
    - model_path: Qwen/Qwen3-30B-A3B-Instruct-2507
      engine_num: 1
      tensor_parallel_size: 2
      enable_thinking: false
      max_prompt_tokens: 19456
      max_response_tokens: 1024
      max_model_len: 20480
```
è¯·å‚è€ƒä½¿ç”¨ LLM-as-a-judge çš„[é…ç½®](https://github.com/modelscope/Trinity-RFT/blob/main/examples/grpo_rubric_as_reward/rubric.yaml)å’Œ[å·¥ä½œæµ](https://github.com/modelscope/Trinity-RFT/blob/main/trinity/common/workflows/rubric_judge_workflow.py)äº†è§£æ›´å¤šè¯¦æƒ…ã€‚


### Trainer

| veRL | Trinity-RFT | è¯´æ˜ |
|:-----|:-----|:-----|
| `trainer.logger` | `monitor.monitor_type` | æ”¯æŒé€‰æ‹©çš„ç±»å‹å’Œï¼ˆæ— éœ€è®¾ç½®ï¼‰`console` |
| `trainer.project_name` | `project` | - |
| `trainer.experiment_name` | `name` | - |
| `trainer.default_local_dir` | `checkpoint_root_dir` | æ£€æŸ¥ç‚¹ä¿å­˜åœ¨ `<checkpoint_root_dir>/<project>/<name>/` |
| `trainer.n_gpus_per_node` | `cluster.gpu_per_node` | - |
| `trainer.nnodes` | `cluster.node_num` | - |
| `trainer.save_freq` | `trainer.save_interval` | - |
| `trainer.test_freq` | `explorer.eval_interval` | - |
| `trainer.total_epochs` | `buffer.total_epochs` | - |
| `trainer.total_training_steps` | `buffer.total_steps` å’Œ `trainer.total_steps` | å¦‚æœä¸ä¸º Noneï¼Œå°†å¿½ç•¥ `buffer.total_epochs` |
| `trainer.critic_warmup` | `trainer.trainer_config.trainer.critic_warmup` | - |
| `trainer.val_before_train` | `explorer.eval_on_startup` | - |
| `trainer.resume_mode` | `continue_from_checkpoint` | ç¨åè¯´æ˜ |
| `trainer.resume_from_path` | - | ç¨åè¯´æ˜ |

ğŸ’¡ è¯¦ç»†è¯´æ˜ï¼š

* å¦‚æœæ‚¨æƒ³ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Œå¯ä»¥å°† `continue_from_checkpoint` è®¾ç½®ä¸º `True`ï¼Œè®­ç»ƒå°†ä»æ£€æŸ¥ç‚¹è·¯å¾„ `<checkpoint_root_dir>/<project>/<name>/` ä¸­çš„æœ€æ–°æ£€æŸ¥ç‚¹å¼€å§‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚


## GPU èµ„æºåˆ†é…

åœ¨ Trinity-RFT ä¸­ï¼ŒGPU èµ„æºéœ€è¦æ‰‹åŠ¨åˆ†é…ç»™ `explorer`ã€`auxiliary models`ï¼ˆå¦‚æœæœ‰ï¼‰å’Œ `trainer`ã€‚

* æ€»å…±æœ‰ `cluster.node_num` ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ‰ `cluster.gpu_per_node` ä¸ª GPUã€‚
* `explorer` ä½¿ç”¨çš„ GPU æ•°é‡ä¸º `explorer.rollout_model.engine_num` * `explorer.rollout_model.tensor_parallel_size`ã€‚
* è¾…åŠ©æ¨¡å‹çš„ GPU æ•°é‡ä¸º `explorer.auxiliary_models[i].engine_num` * `explorer.auxiliary_models[i].tensor_parallel_size`ã€‚
* å‰©ä½™çš„ GPU ç”¨äº `trainer`ã€‚


## æŒ‡æ ‡æ˜ å°„

### ä¸ºä»€ä¹ˆæ¯ä¸ªå®éªŒä¼šçœ‹åˆ°ä¸¤ä¸ªè¿è¡Œè®°å½•ï¼Ÿ

åœ¨ Trinity-RFT ä¸­ï¼Œexplorer è´Ÿè´£ rollout è¿‡ç¨‹ï¼Œè€Œ trainer è´Ÿè´£è®­ç»ƒè¿‡ç¨‹ã€‚è¿™ä¸¤ä¸ªè¿‡ç¨‹çš„æŒ‡æ ‡æ˜¯ç‹¬ç«‹è®¡ç®—çš„ï¼Œå¹¶ä½œä¸ºå•ç‹¬çš„è¿è¡Œä¸Šä¼ åˆ° monitorã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ‚¨ä¼šçœ‹åˆ°æ¯ä¸ªå®éªŒä¼šå¯¹åº”ä¸¤ä¸ªâ€œrunâ€ï¼Œé€šè¿‡ "_explorer" æˆ– "_trainer" åç¼€æ¥åŒºåˆ†ã€‚


### ä¸ºä»€ä¹ˆæŸäº›æŒ‡æ ‡ä¸ veRL ä¸åŒï¼Ÿ

Trinity-RFT ä½¿ç”¨ [vllm](https://github.com/vllm-project/vllm) ä½œä¸º rollout å¼•æ“ï¼Œä½¿ç”¨ veRL ä½œä¸ºè®­ç»ƒåç«¯ã€‚ç”±äºè¿™äº›æ¡†æ¶ä¹‹é—´çš„ç²¾åº¦å·®å¼‚ï¼Œåœ¨ç»™å®š token ä¸Šè®¡ç®—çš„å¯¹æ•°æ¦‚ç‡å¯èƒ½ä¸åŒã€‚å› æ­¤ï¼ŒæŸäº›æŒ‡æ ‡ï¼ˆä¾‹å¦‚ `actor/ppo_kl` å’Œ `actor/pg_clipfrac`ï¼‰å¯èƒ½ä¸ veRL ä¸­è§‚å¯Ÿåˆ°çš„ä¸åŒã€‚ä½†æ˜¯ï¼Œå½“ä½¿ç”¨ä¸ veRL ç›¸åŒçš„å‚æ•°æ—¶ï¼Œè¿™äº›å·®å¼‚é¢„è®¡ä¼šå¾ˆå°ã€‚


## ç¤ºä¾‹ï¼šPPO è®­ç»ƒ

æˆ‘ä»¬å°†ä¸€ä¸ª PPO è®­ç»ƒç¤ºä¾‹ `run_qwen2-7b_rm.sh` ä» veRL çš„é…ç½®è½¬æ¢ä¸º Trinity-RFT çš„é…ç½®ã€‚

veRL çš„é…ç½®å¦‚ä¸‹ï¼š
```bash
gsm8k_train_path=$HOME/data/gsm8k/train.parquet
gsm8k_test_path=$HOME/data/gsm8k/test.parquet
math_train_path=$HOME/data/math/train.parquet
math_test_path=$HOME/data/math/test.parquet

train_files="['$gsm8k_train_path', '$math_train_path']"
test_files="['$gsm8k_test_path', '$math_test_path']"

# prepare model ckpt
huggingface-cli download Qwen/Qwen2-7B-Instruct --local-dir $HOME/models/Qwen2-7B-Instruct &
huggingface-cli download sfairXC/FsfairX-LLaMA3-RM-v0.1 --local-dir $HOME/models/FsfairX-LLaMA3-RM-v0.1 &
wait

python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    data.train_files="$train_files" \
    data.val_files="$test_files" \
    data.train_batch_size=1024 \
    data.max_prompt_length=1024 \
    data.max_response_length=512 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    data.return_raw_chat=True \
    actor_rollout_ref.model.path="$HOME/models/Qwen2-7B-Instruct" \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.1 \
    actor_rollout_ref.actor.ppo_mini_batch_size=256 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16 \
    actor_rollout_ref.actor.use_kl_loss=False \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=16 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
    critic.optim.lr=1e-5 \
    critic.model.use_remove_padding=True \
    critic.optim.lr_warmup_steps_ratio=0.05 \
    critic.model.path="$HOME/models/Qwen2-7B-Instruct" \
    critic.model.enable_gradient_checkpointing=True \
    critic.ppo_micro_batch_size_per_gpu=32 \
    critic.model.fsdp_config.param_offload=False \
    critic.model.fsdp_config.optimizer_offload=False \
    reward_model.enable=True \
    reward_model.model.path="$HOME/models/FsfairX-LLaMA3-RM-v0.1" \
    reward_model.model.use_remove_padding=True \
    reward_model.model.fsdp_config.param_offload=True \
    reward_model.micro_batch_size_per_gpu=32 \
    algorithm.use_kl_in_reward=False \
    trainer.critic_warmup=0 \
    trainer.logger='["console","wandb"]' \
    trainer.project_name='verl_example' \
    trainer.val_before_train=False \
    trainer.experiment_name='Qwen2-7B-Instruct_hybrid_rm' \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes=1 \
    trainer.save_freq=20 \
    trainer.test_freq=5 \
    trainer.total_epochs=15 $@
```

Trinity-RFT çš„ç›¸åº”é…ç½®ï¼ˆppo_example.yamlï¼‰å¦‚ä¸‹ï¼š
```yaml
project: verl_example
name: Qwen2-7B-Instruct_hybrid_rm
checkpoint_root_dir: ./checkpoints
algorithm:
  algorithm_type: ppo
  repeat_times: 1
  optimizer:
    lr: 1e-6
    lr_warmup_steps_ratio: 0.1  # actor_rollout_ref.actor.optim.lr_warmup_steps_ratio
  advantage_fn: ppo  # algorithm.adv_estimator=gae
  kl_penalty_fn: none  # algorithm.use_kl_in_reward=False
  kl_loss_fn: none  # actor_rollout_ref.actor.use_kl_loss=False

model:
  model_path: ${oc.env:HOME}/models/Qwen2-7B-Instruct
  critic_model_path: ${oc.env:HOME}/models/Qwen2-7B-Instruct  # critic.model.path
  max_prompt_tokens: 1024  # data.max_prompt_length
  max_response_tokens: 512  # data.max_response_length
  enable_prompt_truncation: true  # data.filter_overlong_prompts=True

cluster:
  node_num: 1  # trainer.nnodes
  gpu_per_node: 8  # trainer.n_gpus_per_node

buffer:
  total_epochs: 15  # trainer.total_epochs
  batch_size: 256  # actor_rollout_ref.actor.ppo_mini_batch_size
  train_batch_size: 256  # actor_rollout_ref.actor.ppo_mini_batch_size * actor_rollout_ref.rollout.n=256*1=256
  explorer_input:
    tasksets:
      - name: gsm8k
        storage_type: file
        path: ${oc.env:HOME}/data/gsm8k
        split: train
        format:
          prompt_key: prompt  # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
          response_key: answer # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
      - name: math
        storage_type: file
        path: ${oc.env:HOME}/data/math
        split: train
        format:
          prompt_key: prompt  # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
          response_key: answer # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
        rollout_args:
          temperature: 1.0
    eval_tasksets:
      - name: gsm8k_eval
        storage_type: file
        path: ${oc.env:HOME}/data/gsm8k
        split: test
        format:
          prompt_key: prompt  # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
          response_key: answer # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
      - name: math_eval
        storage_type: file
        path: ${oc.env:HOME}/data/math
        split: test
        format:
          prompt_key: prompt  # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
          response_key: answer # æ£€æŸ¥æ•°æ®é›†æ ¼å¼

explorer:
  eval_interval: 5  # trainer.test_freq
  eval_on_startup: false  # trainer.val_before_train=False
  rollout_model:
    engine_num: 2 # rollout æ¨¡å‹çš„ GPU æ•°é‡
    tensor_parallel_size: 1  # actor_rollout_ref.rollout.tensor_model_parallel_size
    gpu_memory_utilization: 0.6  # actor_rollout_ref.rollout.gpu_memory_utilization
  auxiliary_models:  # reward_model é…ç½®
    - model_path: ${oc.env:HOME}/models/FsfairX-LLaMA3-RM-v0.1
      engine_num: 2 # å¥–åŠ±æ¨¡å‹çš„ GPU æ•°é‡
      tensor_parallel_size: 1

synchronizer:
  sync_style: fixed
  sync_offset: 1
  sync_interval: 4  # sync_interval = data.train_batch_size / actor_rollout_ref.actor.ppo_mini_batch_size
  sync_timeout: 1200

trainer:
  save_interval: 20  # trainer.save_freq
  trainer_config:
    actor_rollout_ref:
      model:
        use_remove_padding: true  # actor_rollout_ref.model.use_remove_padding
        enable_gradient_checkpointing: true  # actor_rollout_ref.model.enable_gradient_checkpointing
      actor:
        ppo_micro_batch_size_per_gpu: 16  # actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu
        fsdp_config:
          param_offload: false  # actor_rollout_ref.actor.fsdp_config.param_offload
          optimizer_offload: false  # actor_rollout_ref.actor.fsdp_config.optimizer_offload
      rollout:
        log_prob_micro_batch_size_per_gpu: 16  # actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu
    critic:
      model:
        use_remove_padding: true  # critic.model.use_remove_padding
        enable_gradient_checkpointing: true  # critic.model.enable_gradient_checkpointing
        fsdp_config:
          param_offload: false  # critic.model.fsdp_config.param_offload
          optimizer_offload: false  # critic.model.fsdp_config.optimizer_offload
      optim:
        lr: 1e-5  # critic.optim.lr
        lr_warmup_steps_ratio: 0.05  # critic.optim.lr_warmup_steps_ratio
      ppo_micro_batch_size_per_gpu: 32  # critic.ppo_micro_batch_size_per_gpu
    trainer:
      critic_warmup: 0  # trainer.critic_warmup

monitor:
  monitor_type: wandb  # trainer.logger='["console","wandb"]' - wandb æ˜¯è®¾å®šå€¼ï¼Œconsole æ˜¯é»˜è®¤å€¼
```

è¿è¡Œå‘½ä»¤ä¸ºï¼š
```bash
trinity run --config ppo_example.yaml
```

## ç¤ºä¾‹ï¼šGRPO è®­ç»ƒ

æˆ‘ä»¬å°†ä¸€ä¸ª GRPO è®­ç»ƒç¤ºä¾‹ `run_deepseek7b_llm_seq_balance.sh` ä» veRL çš„é…ç½®è½¬æ¢ä¸º Trinity-RFT çš„é…ç½®ã€‚

veRL çš„é…ç½®å¦‚ä¸‹ï¼š
```bash
set -x

python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=1024 \
    data.max_prompt_length=512 \
    data.max_response_length=512 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    actor_rollout_ref.model.path=deepseek-ai/deepseek-llm-7b-chat \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=256 \
    actor_rollout_ref.actor.use_dynamic_bsz=True \
    actor_rollout_ref.actor.ppo_max_token_len_per_gpu=24000 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
    actor_rollout_ref.rollout.n=8 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    algorithm.use_kl_in_reward=False \
    trainer.critic_warmup=0 \
    trainer.logger='["console","wandb"]' \
    trainer.project_name='verl_grpo_example_gsm8k' \
    trainer.experiment_name='deepseek_llm_7b_function_rm_seq_packing' \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes=1 \
    trainer.save_freq=20 \
    trainer.test_freq=5 \
    trainer.total_epochs=15 $@
```

Trinity-RFT çš„ç›¸åº”é…ç½®ï¼ˆgrpo_example.yamlï¼‰å¦‚ä¸‹ï¼š
```yaml
project: verl_grpo_example_gsm8k
name: deepseek_llm_7b_function_rm_seq_packing
checkpoint_root_dir: ./checkpoints
algorithm:
  algorithm_type: grpo
  repeat_times: 8  # actor_rollout_ref.rollout.n=8
  optimizer:
    lr: 1e-6  # actor_rollout_ref.actor.optim.lr
  advantage_fn: grpo  # algorithm.adv_estimator=grpo
  kl_penalty_fn: none  # algorithm.use_kl_in_reward=False
  kl_loss_fn: low_var_kl  # actor_rollout_ref.actor.kl_loss_type=low_var_kl
  kl_loss_fn_args:
    kl_coef: 0.001  # actor_rollout_ref.actor.kl_loss_coef
  entropy_loss_fn_args:
    entropy_coef: 0  # actor_rollout_ref.actor.entropy_coeff=0

model:
  model_path: deepseek-ai/deepseek-llm-7b-chat  # actor_rollout_ref.model.path
  max_prompt_tokens: 512  # data.max_prompt_length
  max_response_tokens: 512  # data.max_response_length
  enable_prompt_truncation: true  # data.filter_overlong_prompts=True

cluster:
  node_num: 1  # trainer.nnodes
  gpu_per_node: 8  # trainer.n_gpus_per_node

buffer:
  total_epochs: 15  # trainer.total_epochs
  batch_size: 256  # actor_rollout_ref.actor.ppo_mini_batch_size
  train_batch_size: 2048  # actor_rollout_ref.actor.ppo_mini_batch_size * actor_rollout_ref.rollout.n=256*8=2048
  explorer_input:
    tasksets:
      - name: gsm8k
        storage_type: file
        path: ${oc.env:HOME}/data/gsm8k
        split: train
        format:
          prompt_key: prompt  # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
          response_key: answer  # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
    eval_tasksets:
      - name: gsm8k_eval
        storage_type: file
        path: ${oc.env:HOME}/data/gsm8k
        split: test
        format:
          prompt_key: prompt  # æ£€æŸ¥æ•°æ®é›†æ ¼å¼
          response_key: answer  # æ£€æŸ¥æ•°æ®é›†æ ¼å¼

explorer:
  eval_interval: 5  # trainer.test_freq
  rollout_model:
    engine_num: 1
    tensor_parallel_size: 2  # actor_rollout_ref.rollout.tensor_model_parallel_size
    gpu_memory_utilization: 0.6  # actor_rollout_ref.rollout.gpu_memory_utilization

synchronizer:
  sync_style: fixed
  sync_offset: 1
  sync_interval: 4  # veRL ä¸­çš„ data.train_batch_size / actor_rollout_ref.actor.ppo_mini_batch_size
  sync_timeout: 1200

trainer:
  save_interval: 20  # trainer.save_freq
  use_dynamic_bsz: true  # actor_rollout_ref.actor.use_dynamic_bsz=True
  max_token_len_per_gpu: 24000  # actor_rollout_ref.actor.ppo_max_token_len_per_gpu
  trainer_config:
    actor_rollout_ref:
      model:
        use_remove_padding: true  # actor_rollout_ref.model.use_remove_padding=True
        enable_gradient_checkpointing: true  # actor_rollout_ref.model.enable_gradient_checkpointing=True
      actor:
        fsdp_config:
          param_offload: false  # actor_rollout_ref.actor.fsdp_config.param_offload=False
          optimizer_offload: false  # actor_rollout_ref.actor.fsdp_config.optimizer_offload=False
      ref:
        fsdp_config:
          param_offload: true  # actor_rollout_ref.ref.fsdp_config.param_offload=True
    trainer:
      critic_warmup: 0  # trainer.critic_warmup=0

monitor:
  monitor_type: wandb  # trainer.logger='["console","wandb"]' - wandb æ˜¯è®¾å®šå€¼ï¼Œconsole æ˜¯é»˜è®¤å€¼
```

è¿è¡Œå‘½ä»¤ä¸ºï¼š
```bash
trinity run --config grpo_example.yaml
```
